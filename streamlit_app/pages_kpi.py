# streamlit_app/pages_kpi.py

import streamlit as st

import pandas as pd

def page_kpi():

    # ============================
    # INTRO
    # ============================
    st.title(
        "Voici les **KPI cl√©s** du territoire, du cin√©ma et du moteur de recommandation"
    )

    # ============================
    # 1) KPI INSEE
    # ============================

    st.subheader("üìä D√©mographie ‚Äî INSEE")

    col1, col2, col3 = st.columns(3)
    col1.metric("Population totale", "118 000")
    col2.metric("60 ans et +", "47 %", "+6 pts depuis 2011")
    col3.metric("Moins de 30 ans", "22 %", "-4 pts depuis 2011")
    col1.metric("M√©nages d'une personne", "41 %")
    col2.metric("Pauvret√© <30 ans", "25 %")

    st.markdown("---")

    # ============================
    # 2) KPI CNC
    # ============================

    st.subheader("üé¨ Cin√©ma ‚Äî CNC")

    col1, col2, col3 = st.columns(3)
    col1.metric("√âcrans (2024)", "4", "-67 % depuis 1966")
    col2.metric("Entr√©es annuelles", "45 000", "Stable")
    col3.metric("Entr√©es / habitant", "0.35", "France : 2.8")
    col1.metric("S√©ances annuelles", "2 000")
    col2.metric("Taux d'occupation", "0.25", "-50 % vs France")

    st.markdown("---")

    # ============================
    # 3) KPI AVANT TRAITEMENT IMDB
    # ============================

    st.header("üì¶ Bases de donn√©es ")

    col1, col2, col3 = st.columns(3)
    col1.metric("Films TMDB (brut)", "309 572")
    col2.metric("NB dataset", "5 fichiers")
    col3.metric("Colonnes TMDB", "40")

    
    



    # ============================
    # 4) KPI APR√àS TRAITEMENT IMDB (sans df)
    # ============================

    

    col1, col2, col3 = st.columns(3)
    col1.metric("Films IMDB (apr√®s traitement)", "38 924" )
    col2.metric( "dataset final", "1 Fichiers")
    col3.metric("Colonnes finales", "9")

    st.markdown("---")

    # ============================
    # 5) KPI TRAITEMENT IMDB (avec df)
    # ============================

   
    @st.cache_data
    def load_features():
        return pd.read_csv("data/data_processed/movies_local.csv.gz")

    df = load_features()

    processing_kpi = {
        "films_total": len(df),
        "genres_valides": df["genres"].notna().mean() * 100,
        "directors_valides": df["director_names"].notna().mean() * 100,
        "casting_valide": df["cast_names_top5"].notna().mean() * 100,
        "runtime_valide": df["runtimeMinutes"].gt(0).mean() * 100,
        "soup_completude": 100.0,
        "longueur_moyenne_soup": 55,
        "vocabulaire_tfidf": "40k‚Äì60k tokens",
    }

   


    # ============================
    # 6) KPI RECOMMANDATION
    # ============================

    st.header("ü§ñ Moteur de recommandation (contenu)")

    reco_kpi = {
        "films_recommandables": len(df),
        "diversite_genres": df["genres"].str.split(",").explode().nunique(),
        "richesse_cast": df["cast_names_top5"].str.split("|").explode().nunique(),
        "temps_reco": "< 50 ms",
        
    }

    col1, col2, col3 = st.columns(3)
    col1.metric("Films recommandables", "38 924")
    col2.metric("Genres uniques", f"{reco_kpi['diversite_genres']}")
    col3.metric("Acteurs uniques", f"{reco_kpi['richesse_cast']:,}")

    
    st.subheader("model choisie : Content-Based Recommender (TF-IDF + Cosine Similarity)")
    st.write("Filtrage bas√© sur le contenu (Content-Based Filtering)")
    st.info(
    "Le syst√®me de recommandation repose sur un filtrage bas√© sur le contenu. "
    "Chaque film est repr√©sent√© par un vecteur TF-IDF construit √† partir de ses m√©tadonn√©es "
    "(genres, r√©alisateurs, acteurs). Les recommandations sont obtenues via une similarit√© cosinus."
            )
    with st.expander("variable"):
        st.code("""
                
                # Content-Based Recommender Model (TF-IDF + Cosine Similarity)
                vectorizer = TfidfVectorizer(...)

                # Similarity-based recommendation using cosine similarity
                sims = cosine_similarity(q_vec, art.matrix)
                
                """)
    st.subheader("extrait du code pour le ML")
    with st.expander("Chargement des artefacts (principe du mod√®le offline / online)"):
        st.code(
        """
        @st.cache_resource(show_spinner=False)
        def load_reco_artifacts() -> RecoArtifacts:
            # Chargement du vectorizer TF-IDF entra√Æn√© hors ligne
            vectorizer = joblib.load(RECO_DIR / "tfidf_vectorizer.joblib")

            # Chargement de la matrice TF-IDF contenant tous les films du catalogue
            # Chaque ligne = un film, chaque colonne = un terme
            matrix = joblib.load(RECO_DIR / "tfidf_matrix.joblib")

            # Chargement de l‚Äôindex des films (tconst dans le m√™me ordre que la matrice)
            idx = pd.read_csv(RECO_DIR / "tconst_index.csv")

            # Liste ordonn√©e des identifiants de films
            tconst_list = idx["tconst"].astype(str).tolist()

            # Dictionnaire pour acc√©der rapidement √† la ligne d‚Äôun film dans la matrice
            # ex: tconst_to_row["tt0133093"] -> index de ligne
            tconst_to_row = {t: i for i, t in enumerate(tconst_list)}

            # Regroupement de tous les artefacts dans une structure unique
            return RecoArtifacts(
                vectorizer=vectorizer,
                matrix=matrix,
                tconst_list=tconst_list,
                tconst_to_row=tconst_to_row,
            )

        """
        )
    
    
    
    with st.expander("Fonction de recommandation principale (film connu)"):
        st.code(
            """ 
            def recommend_by_tconst(query_tconst: str, top_n: int = 10):
                # Chargement des artefacts TF-IDF et de la matrice
                art = load_reco_artifacts()

                # V√©rification que le film existe dans le catalogue
                if query_tconst not in art.tconst_to_row:
                    return []

                # R√©cup√©ration de l‚Äôindex du film dans la matrice
                q_idx = art.tconst_to_row[query_tconst]

                # Vecteur TF-IDF du film cible
                q_vec = art.matrix[q_idx]

                # Calcul de la similarit√© cosinus entre ce film et tous les autres
                sims = cosine_similarity(q_vec, art.matrix).ravel()

                # Exclusion du film lui-m√™me (√©vite l‚Äôauto-recommandation)
                sims[q_idx] = -1.0

                # S√©lection des indices des top-N films les plus similaires
                top_idx = np.argpartition(-sims, top_n)[:top_n]

                # Retourne les tconst recommand√©s avec leur score de similarit√©
                return [(art.tconst_list[i], float(sims[i])) for i in top_idx]           
            
            """     
        )
    
    
    
    with st.expander("Cas film externe"):
        st.code("""
            
               def recommend_by_soup(query_soup: str, top_n: int = 10):
                    # Chargement des artefacts existants
                    art = load_reco_artifacts()

                    # Nettoyage du texte d‚Äôentr√©e
                    query_soup = (query_soup or "").strip().lower()
                    if not query_soup:
                        return []

                    # Transformation du texte en vecteur TF-IDF
                    # IMPORTANT : on utilise le vectorizer existant (pas de refit)
                    q_vec = art.vectorizer.transform([query_soup])

                    # Calcul de la similarit√© cosinus avec tous les films du catalogue
                    sims = cosine_similarity(q_vec, art.matrix).ravel()

                    # S√©lection des top-N films les plus proches
                    top_idx = np.argpartition(-sims, top_n)[:top_n]

                    return [(art.tconst_list[i], float(sims[i])) for i in top_idx]
                """
            )
    
    with st.expander("Construction du TF-IDF offline"):
        st.code("""
                # Cr√©ation du vectorizer TF-IDF
                vectorizer = TfidfVectorizer(
                    max_features=120_000,   # limite la taille du vocabulaire
                    ngram_range=(1, 2),     # mots seuls + paires de mots
                    min_df=2,               # ignore les termes trop rares
                    max_df=0.90             # ignore les termes trop fr√©quents
                )

                # Entra√Ænement sur la colonne "soup" (repr√©sentation textuelle des films)
                X = vectorizer.fit_transform(df["soup"]) 
                """)
    
    

    st.markdown("---")

    # ============================
    # 7) APER√áU DATASET
    # ============================

    st.subheader("Aper√ßu du dataset apr√®s nettoyage")
    st.dataframe(df.head())



    # ============================
    # 7) info
    # ============================
    
    st.markdown("# Notes")
    st.subheader("Traitement des donn√©es & Machine Learning")
    st.markdown(
        """
    ## 1. Sources de donn√©es

    ### Donn√©es IMDB (brutes)

    * `name.basics.tsv.gz`
    * `title.basics.tsv.gz`
    * `title.crew.tsv.gz`
    * `title.principals.tsv.gz`
    * `title.ratings.tsv.gz`

    Ces fichiers constituent la base brute IMDB. Ils **ne sont jamais utilis√©s directement** dans Streamlit.

    ### Donn√©es externes

    * INSEE : d√©mographie, m√©nages, pauvret√©, salaires
    * CNC : √©crans, entr√©es, fr√©quentation cin√©ma
    * TMDB (API) : posters, titres FR, synopsis, popularit√©

    ---

    ## 2. Nettoyage et pr√©paration des donn√©es (offline)

    ### Objectif

    Construire un **catalogue films propre, l√©ger et exploitable** pour la recommandation et l‚Äôaffichage.

    ### √âtapes principales

    1. **Filtrage des films**

    * Suppression des contenus non-films (s√©ries, √©pisodes)
    * Filtrage temporel (films r√©cents / pertinents)
    * Seuils de votes pour garantir une base fiable

    2. **Nettoyage des champs**

    * Harmonisation des genres
    * Suppression des valeurs manquantes critiques
    * Normalisation des titres et identifiants

    3. **Construction des fichiers finaux**

    * `movies_local.csv.gz`

        * Identifiant IMDb (`tconst`)
        * Titre principal
        * Ann√©e de sortie
        * Genres
        * Donn√©es utiles √† l‚Äôaffichage

    * `movies_features.csv.gz`

        * Identifiant IMDb
        * Texte descriptif ("soup") pour le ML

    üëâ Ces √©tapes sont r√©alis√©es via des **scripts Python offline** (`scripts/`), jamais sur Streamlit Cloud.

    ---

    ## 3. Feature Engineering pour la recommandation

    ### Principe de la "soup"

    Chaque film est repr√©sent√© par un texte combinant :

    * Genres
    * R√©alisateur
    * Acteurs principaux

    Exemple (simplifi√©) :

    ```
    Drama Thriller nolan dicaprio hardy
    ```

    Ce format permet une vectorisation simple et efficace.

    ---

    ## 4. Mod√®le de recommandation

    ### 4.1 Logique g√©n√©rale du Machine Learning

    Le syst√®me de recommandation repose sur un **mod√®le de type Content-Based Filtering**. Le principe est de repr√©senter chaque film sous forme vectorielle, √† partir de ses caract√©ristiques textuelles, puis de mesurer la similarit√© entre films.

    L‚Äôint√©gralit√© de la phase d‚Äôapprentissage est r√©alis√©e **en amont (offline)** afin de garantir de bonnes performances dans l‚Äôapplication Streamlit.

    ---

    ### 4.2 Pr√©paration des donn√©es pour le ML (en amont)

    √Ä partir de la base nettoy√©e issue d‚ÄôIMDB, une table d√©di√©e au Machine Learning est construite (`movies_features.csv.gz`).

    Pour chaque film, on g√©n√®re une **repr√©sentation textuelle unique appel√©e "soup"**, qui agr√®ge :

    * les genres du film
    * le r√©alisateur principal
    * les acteurs principaux

    Exemple de soup :

    ```
    Drama Thriller nolan dicaprio hardy
    ```

    Cette √©tape est cruciale : elle permet de transformer des donn√©es h√©t√©rog√®nes (cat√©gories, noms propres) en une forme exploitable par un mod√®le NLP simple.

    ---

    ### 4.3 Vectorisation TF-IDF (fit offline)

    Une fois la colonne "soup" construite pour l‚Äôensemble du catalogue :

    1. Un **TF-IDF Vectorizer** est entra√Æn√© sur l‚Äôint√©gralit√© des soups du catalogue
    2. Chaque film est transform√© en un **vecteur num√©rique** de dimension √©lev√©e

    Ce processus produit :

    * une matrice creuse TF-IDF (films √ó termes)
    * un vocabulaire pond√©r√© par l‚Äôimportance des mots

    Les fichiers g√©n√©r√©s sont :

    * `tfidf_vectorizer.joblib` ‚Üí le mod√®le TF-IDF entra√Æn√© (fit)
    * `tfidf_matrix.joblib` ‚Üí la matrice vectoris√©e des films
    * `tconst_index.csv` ‚Üí mapping entre identifiant IMDb et index de ligne

    Ces artefacts sont sauvegard√©s sur disque et **ne sont jamais recalcul√©s dans Streamlit**.

    ---

    ### 4.4 Chargement et utilisation dans Streamlit

    Dans l‚Äôapplication :

    * les artefacts sont charg√©s une seule fois via `st.cache_resource`
    * la matrice et le vectorizer restent en m√©moire pour toutes les sessions

    Deux cas d‚Äôusage sont alors possibles.

    ---

    ### 4.5 Recommandation √† partir d‚Äôun film du catalogue

    Lorsque l‚Äôutilisateur s√©lectionne un film d√©j√† pr√©sent dans la base locale :

    1. On r√©cup√®re son index dans la matrice TF-IDF
    2. On calcule la **similarit√© cosinus** entre son vecteur et tous les autres films
    3. On trie les scores et on retourne les films les plus similaires

    Ce m√©canisme est rapide car il s‚Äôappuie uniquement sur des calculs matriciels en m√©moire.

    ---

    ### 4.6 Recommandation √† partir d‚Äôun film externe (API TMDB)

    Pour un film **absent du catalogue local** (par exemple issu de la recherche TMDB) :

    1. Les informations du film sont r√©cup√©r√©es via l‚ÄôAPI TMDB
    2. Une soup est construite dynamiquement (genres + r√©alisateur + acteurs)
    3. Cette soup est **transform√©e** avec le vectorizer existant (pas de refit)
    4. Le vecteur obtenu est compar√© √† la matrice TF-IDF locale via similarit√© cosinus

    üëâ Le mod√®le n‚Äôest jamais r√©entra√Æn√© : on applique uniquement une **transformation** coh√©rente avec l‚Äôapprentissage initial.

    ---

    ### 4.7 Coh√©rence entre donn√©es locales et donn√©es API

    Le point cl√© du syst√®me est la **coh√©rence du pipeline** :

    * m√™me logique de soup
    * m√™me normalisation (minuscules, espaces)
    * m√™me vectorizer

    Cela garantit que les films issus de l‚ÄôAPI TMDB sont projet√©s dans **le m√™me espace vectoriel** que les films du catalogue local.

    ---

    ### 4.8 Pourquoi ce choix de mod√®le

    Ce mod√®le a √©t√© choisi car il :

    * est explicable
    * ne n√©cessite pas de donn√©es utilisateurs
    * est rapide et robuste
    * est parfaitement adapt√© √† un contexte Data Analyst

    Il permet de d√©montrer une cha√Æne ML compl√®te sans complexit√© inutile.

    ---

    ### 4.9 Limites sp√©cifiques du ML

    * Pas de prise en compte des pr√©f√©rences utilisateurs
    * Sensible √† la qualit√© des m√©tadonn√©es (genres, casting)
    * Ne capture pas les relations s√©mantiques profondes

    ---

    ### 4.10 √âvolutions possibles

    * Passage √† des embeddings (Word2Vec, SBERT)
    * Ajout d‚Äôun scoring hybride (contenu + popularit√©)
    * Int√©gration de feedback utilisateur

    ### Type de mod√®le

    * **Content-Based Filtering**
    * Aucun apprentissage supervis√©
    * Pas de donn√©es utilisateur

    ### M√©thode

    1. Vectorisation TF-IDF sur la soup
    2. Calcul de similarit√© cosinus entre films
    3. Classement des films les plus proches

    ### Artefacts produits

    * `tfidf_vectorizer.joblib`
    * `tfidf_matrix.joblib`
    * `tconst_index.csv`

    Ces fichiers sont charg√©s **une seule fois** dans Streamlit gr√¢ce √† `st.cache_resource`.

    ---

    ## 5. Int√©gration TMDB (enrichissement)

    ### R√¥le de TMDB

    * Titres en fran√ßais
    * Posters et backdrops
    * Synopsis
    * Popularit√©

    ### Fonctionnement

    * Appels API encapsul√©s dans `tmdb_client.py`
    * Cache disque + cache Streamlit
    * Aucun enrichissement massif au chargement

    ### Principe cl√©

    üëâ **L‚Äôenrichissement se fait uniquement √† l‚Äôaffichage** (Top 5 / cartes visibles)

    Cela garantit :

    * Performance
    * Respect des quotas API

    ---

    ## 6. Reranking et contextualisation

    Pour certains cas (films √† l‚Äôaffiche / √† venir) :

    * Construction de sets IMDb `now_playing` / `upcoming`
    * Permet d‚Äôannoter ou prioriser les recommandations

    Ces calculs sont :

    * Mis en cache
    * Recalcul√©s √† intervalle contr√¥l√© (TTL)

    ---

    ## 7. Architecture Streamlit (r√©sum√©)

    * **Offline** : nettoyage, feature engineering, ML
    * **Online (Streamlit)** :

    * Chargement des fichiers finaux
    * Recommandation en temps r√©el
    * Enrichissement visuel √† la demande

    Cette s√©paration garantit :

    * Performance
    * Reproductibilit√©
    * Scalabilit√©

    ---

    ## 8. Limites identifi√©es

    * Pas de personnalisation utilisateur
    * Recommandation bas√©e uniquement sur le contenu
    * D√©pendance partielle √† une API externe (TMDB)

    ---

    ## Conclusion

    Le projet met en ≈ìuvre une **cha√Æne compl√®te de data analysis appliqu√©e** :

    * collecte
    * nettoyage
    * feature engineering
    * machine learning
    * d√©ploiement applicatif

    Le tout dans une architecture **adapt√©e √† un contexte Data Analyst**, claire, performante et justifiable.


            """
        )
    st.markdown(
            """
            # üìä KPI ‚Äì Traitement des donn√©es

            """        
        )